---
title: "Chapter 14 Count Data | Chapter 15 Count Data in Tables"
author: "Qianqian Shan"
date: "June 5, 2017"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "therbookdata")
```

# Chapter 14 Count Data 

Reason why linear regression not appropriate: 
1. linear regression may lead to negative counts 

2. variance of the response variable is likely to increase with the mean 

3. error may not be normally distributed 

4. zeros are difficult to handle in transformations. 



## Regression with Poisson errors 

Introduce zero-inflated distribution for data with a lot of zeros. 

```{r}

options(contrasts = c("contr.treatment", "contr.poly"))
 
clusters<-read.table("clusters.txt", header = TRUE) 
attach(clusters)
head(clusters, 4)
table(Cancers) # a lot of zero values 

# glm with poisson errors 
model1 <- glm(Cancers ~ Distance, family = poisson)
summary(model1)
# Under poisson errors, the residual deviance is equal to the residual degrees of freedoms,
# there is an obvious sign of overdispersion here 

# use quasipoisson instead , check page 563 of "The R book" or Stat520 notes for quasi likeilhood 
# quasilikelihood only specifies the mean-variance relationship up to a proportionality constant 
model2 <- glm(Cancers ~ Distance, family = quasipoisson)
summary(model2)

# show the fitted line on plot 
xv <- seq(0, 100)
yv <- predict(model2, list(Distance = xv))
plot(Cancers ~ Distance, pch = 21, col = "red", bg = "orange")
lines(xv, exp(yv), col = "blue")
# no obvious trend 
# need to use exp(yv) as y as we used log link 

detach(clusters)





# a way to deal with spike at zeros 
# this is an example using beta binomial distribution for a certain data set 
# Y is a random variable that is almost surely 0 when Z = 0
# and distributed Beta-Binomial(n, alpha, beta) when Z = 1. Z ~ Bernoulli(p).
# More details on HW3 of Stat 601 
mydata <- c(rep(0, 400),
            rep(1, 16),
            rep(2, 12),
            rep(3, 12),
            rep(4, 5),
            rep(5, 10),
            rep(6, 3),
            rep(7, 4),
            rep(8, 2)
)

n <- 8

# pmf for the specific data 
zibb.pmf <- function(y, par){
    p <- par[1]
    a <- par[2]
    b <- par[3]
    
if (y == 0)
    return((1 - p) + p * beta(a, n + b) / beta(a, b))

return(p * choose(n, y) * beta(y + a, n - y + b) / beta(a, b)) 
}


# the log likelihood of the above pmf for each y 
zibb.loglik <- function(i, data, par) return(log(zibb.pmf(data[i], par)))

# the overall likelihood 
full.loglik <- function(par, data) {
L <- length(data)
sum <- sum(sapply(1:L, FUN = zibb.loglik, data = data, par = par)) 
return(sum)
}

# starting values 
startingpar <- c(.5, 1, 1)

# use optim function to find the estimated paramters 
results <- optim(par = startingpar, fn = full.loglik, data = mydata,
                 method = "Nelder-Mead", control = list(fnscale = -1)) 
# By default optim performs minimization, but it will maximize if control$fnscale is negative.
estpars <- results$par
estpars
p <- estpars[1]
a <- estpars[2]
b <- estpars[3]

# calculate the predicted frequencies 
y <- 0:8 

f<- numeric(length(y))
f[1] <- (1- p) + p * beta(a, n + b)/beta(a, b)

for(i in 2:9){
  f[i] <- p * choose(n, i - 1) * beta(i - 1 + a, n - (i - 1) + b)/ beta(a, b)
}

f * sum(table(mydata))
data <- data.frame(observed = table(mydata), predicted = f * (sum(table(mydata))))
data
rm(list = c("y", "n"))
```



## Analysis of deviance with count data 


```{r eval = FALSE}
# no data file found for this chunk 
count <- read.table("cellcounts.txt", header = TRUE) 
attach(count)
names(count)

table(cells)

tapply(cells, smoker, mean)

tapply(cells, weight, mean)

tapply(cells, sex, mean)

tapply(cells, age, mean)

model1 <- glm(cells ~ smoker * sex * age * weight, family = poisson)
summary(model1)


model2 <- glm(cells ~ smoker * sex * age * weight, family = quasipoisson)
summary(model2)

model3 <- update(model2, ~. -smoker:sex:age:weight)
summary(model3)


newWt <- weight
levels(newWt)[c(1, 3)] <- "not"
summary(model15)

tapply(cells, list(smoker, weight), mean)

barplot(tapply(cells, list(smoker, weight), mean), col = c("wheat2", "wheat4"),
        beside = TRUE, ylab = "damaged cells", xlab = "body mass")
legend(1.2, 3.4, c("non-smoker", "smoker"), fill = c("wheat2", "wheat4"))

detach(count)

```


## Analysis of covariance with count data 


```{r}

species1 <- read.table("species.txt", header = TRUE)
attach(species1)
names(species1)

plot(Biomass, Species, type = "n")

# split divides the data in the vector x into the groups defined by f. 
# split(x, f, drop = FALSE, ...)

spp <- split(Species, pH)
spp
bio <- split(Biomass, pH)
bio

points(bio[[1]], spp[[1]], pch = 16, col = "red")
points(bio[[2]], spp[[2]], pch = 16, col = "green")
points(bio[[3]], spp[[3]], pch = 16, col = "blue")
legend("topright", legend = c("high", "low", "medium"),
      pch = c(16, 16, 16), col = c("red", "green", "blue"),
      title = "pH")


# check the main effects and the interaction effects 
model1 <- glm(Species ~ Biomass * pH, family = poisson)
summary(model1)
# no evidence of overdispersion 


# check  if different slopes for different pHs are necessary or not 
model2 <- glm(Species ~ Biomass + pH, family = poisson)
summary(model2)
anova(model1, model2, test = "Chi")
# yes, slopes are significantly different 


# draw fitted lines 
levels(pH)

pHs <- factor(rep("high", 101))
xv <- seq(0, 10, 0.1)
yv <- predict(model1, list(Biomass = xv, pH = pHs))
# draw line for high pH level 
lines(xv, exp(yv), col = "red")

# low 
pHs <- factor(rep("low", 101))
yv <- predict(model1, list(Biomass = xv, pH = pHs))
lines(xv, exp(yv), col = "green")

# mid 
pHs <- factor(rep("mid", 101))
yv <- predict(model1, list(Biomass = xv, pH = pHs))
lines(xv, exp(yv), col = "blue")

detach(species1)
```


## Frequency distribution 

Negative binoamial distribution is used, one parameter is the mean number of cases, the other parameter is the clumping parameter $k$(the degree of aggregation in the data, small $k$ values show high aggregation). 

With an approximate estimate of the magnitude of $k$ : $\hat{k}= \frac{\overline{x}^2}{s^2 - \overline{x}}$. 

```{r}

case.book <- read.table("cases.txt", header = TRUE)
attach(case.book)
names(case.book)

frequencies <- table(cases)
frequencies # a lot of zeros 

mean(cases)

par(mfrow = c(1, 2))

barplot(frequencies, ylab = "Frequency", xlab = "Cases", col = "green4", main = "Observed Cases")
barplot(dpois(0:10, 1.775) * 80, names = as.character(0:10),
        ylab = "Frequency", xlab = "Cases", col = "green3", main = "Theoretical Poisson")

par(mfrow = c(1, 1))
# modes are different , i.e., the observed data are highly aggregated 

var(cases)/mean(cases)

# k value for negative binomial distribution 
mean(cases)^2/(var(cases) - mean(cases))

expected <- dnbinom(0:10, 1, mu = 1.775) * 80
# 1 is the number of success 


# plot observed and expected 
both <- numeric(22)
both[1:22 %% 2 != 0] <- frequencies
both[1:22 %% 2 == 0] <- expected

labels <- character(22)
labels[1:22 %% 2 == 0] <- as.character(0:10)

barplot(both, col = rep(c("red4", "blue4"), 11), names = labels, ylab = "Frequency", xlab ="Cases")

legend("topright", legend = c("observed", "expected"), fill = c("red4", "blue4"))

expected # accumulate the last six frequencies for all values bigger than 4 
# then do Pearson's chi-square test for lack of fit 


# accumulate the last six frequencies 
cs <- factor(0:10)
levels(cs)[6:11] <- "5+"
levels(cs)

(ef <- as.vector(tapply(expected, cs, sum)))
(of <- as.vector(tapply(frequencies, cs, sum)))

chi.statistic <- sum((of - ef)^2/ef)

# df is the number of legitimate comparisons(6) minus the number of parameters 
# estimated from the data(2) , minus 1 
1 - pchisq(chi.statistic, 3)

detach(case.book)
```


## Overdispersion in log-linear models 


```{r}

library(MASS)
data(quine)
attach(quine)
names(quine)
str(quine) # all factors except for response variable 


# maximal model 
model1 <- glm(Days ~ Eth * Sex * Age * Lrn, family = poisson)
summary(model1) # overdispersion 


# fit quasi poisson model 
model2 <- glm(Days ~ Eth * Sex * Age * Lrn, family = quasipoisson)
summary(model2)


# ftable(table(Eth, Sex, Age, Lrn))

# AIC is not defined for this model and thus step function for mdoel selection is not available 
# remove the Age by Lrn interaction from model 2 
model4 <- update(model2, ~.-Age:Lrn)
summary(model4)
anova(model2, model4, test = "F")
anova(model2, model4)

ftable(tapply(Days, list(Eth, Sex, Lrn), mean))

```

## Negative binomial errors 

Use `glm.nb` function and `MASS` package. 


```{r}
# 
model.nb1 <- glm.nb(Days ~ Eth * Sex * Age * Lrn)
summary(model.nb1, cor = FALSE)
# theta in the model summary is the k parameter 

model.nb2 <- stepAIC(model.nb1)
summary(model.nb2, cor = F)

# further simplify the model from above 
model.nb3 <- update(model.nb2, ~. - Sex:Age:Lrn)
anova(model.nb3, model.nb2)

# 
model.nb4 <- update(model.nb3, ~. - Eth:Age:Lrn)
anova(model.nb3, model.nb4)

# 
model.nb5 <- update(model.nb4, ~. - Age:Lrn)
anova(model.nb4, model.nb5)

summary(model.nb5, cor=F)

par(mfrow = c(2, 2))
plot(model.nb5)
par(mfrow = c(1, 1))

detach(quine)
```




# Chapter 15 Count Data in Tables 

The general method of analysis for contingency tables involves log-linear modeling, but the simplest contingency tables are often analyzed by Pearson's Chi-square, Fisher's exact test or tests of binomial proportions.  



## A two-class table of counts 

Pearson's chi-square $\chi^2 = \sum\frac{(observed - expected)^2}{expected}$. 


```{r}

# test if the sex ratio is significant from 50:50 or not 

observed <- c(29, 18)
chisq.test(observed) # not significant 
# performs chi-squared contingency table tests and goodness-of-fit tests.

# or try binomial test alternatively 
binom.test(observed)
``` 

## Sample size for count data 

Test how many samples are needed for detect a significant departure from p = 0.5. 

```{r}
binom.test(1, 8) # n =8 not significant 
binom.test(1, 9) # 9 is significant 
``` 

## A four-class table of counts 


```{r}
# Mendel's famous peas produced 315 yellow round phenotypes 
# 101 yellow wrinkled 
# 108 green round 
# 32 green wrinkled 

# test if the data depart significantly from 9:3:3:1 
observed <- c(315, 101, 108, 32)

(expected <- 556 * c(9, 3, 3, 1)/16)

chisq.test(observed, p = c(9, 3, 3, 1), rescale.p = TRUE)
# rescale is true as the expected values don't sum to 1 
# p-value = 0.9254 , not significant 

# or calculate it by hand 
sum((observed-expected)^2/expected)
1 - pchisq(0.470024, 3)
```


## Two-by-two contingency tables 


When there are two explanatory variables and both have just two levels, we have the famous 2 by 2 contingency table. 


```{r}
# convert the vector into a matrix 
observed <- matrix(observed, nrow = 2)
observed

# Fisher's exact test 
fisher.test(observed)


# Pearson' chi square test 
chisq.test(observed)
```

## Using log-linear models for simple contingency tables 


```{r}
# 29 males and 18 females 
observed <- c(29, 18)

glm(observed ~ 1, family = poisson)

summary(glm(observed ~ 1, family = poisson))

# compare the residual deviance with the critical value of a chisq test 
1 - pchisq(2.5985, 1)


# Mendel's peas : a four level categorical variable 
observed <- c(315, 101, 108, 32)

# two explanatory variables 
shape <- factor(c("round", "round", "wrinkled", "wrinkled"))
colour <- factor(c("yellow", "green", "yellow", "green"))

# maixmal/saturated model 
model1 <- glm(observed ~ shape * colour, family = poisson)
# model w/o interaction

model2 <- glm(observed ~ shape + colour, family = poisson)
anova(model1, model2, test = "Chi") # no significant difference 

summary(model2)
```


## The danger of contingency tables 

Sometimes we may fail to measure a number of factors that have an important influence on the behavior of the system in question. 


```{r}

induced <- read.table("induced.txt", header = TRUE)
attach(induced)
names(induced)

# fit saturated model 
model <- glm(Count ~ Tree * Aphid * Caterpillar, family = poisson)

model2 <- update(model, ~ . - Tree:Aphid:Caterpillar)

anova(model, model2, test = "Chi")


model3 <- update(model2, ~ . - Aphid:Caterpillar)
anova(model3, model2, test = "Chi")


# fit a model without Tree factor 
wrong <- glm(Count ~ Aphid * Caterpillar, family = poisson)
wrong1 <- update (wrong,~. - Aphid:Caterpillar)
anova(wrong, wrong1, test = "Chi") # shows a significant effect of Aphid:Caterpillar, 
# but not in the previous model

detach(induced)
```

**Summary**: always fit a saturated model first, containing all the variables of interest and all interactions. 

## Quasi-Poisson and negative binomial models compared 

```{r}
data <- read.table("bloodcells.txt", header = TRUE) 
attach(data)
head(data)
dim(data)
gender <- factor(rep(c("female", "male"), c(5000, 5000)))

tapply(count, gender, mean)

# fit log-linear model with Poisson errors 
model <- glm(count ~ gender, family = poisson)
summary(model) # gender effects not significant 

# fit quasi Poisson errors 
model <- glm(count ~ gender, family = quasipoisson)
summary(model) # no significant effects 


# negative binoamial error with glm.nb 
library(MASS)
model <- glm.nb(count ~ gender)
summary(model) # p value slightly different 

rm(gender)
detach(data)


```

## A contingency table of intermediate complexity 

```{r}
# three dimensional table of count data 
numbers <- c(24, 30, 29, 41, 14, 31, 36, 35)
dim(numbers) <- c(2, 2, 2)
numbers 


dimnames(numbers)[[3]] <- list("male", "female")
dimnames(numbers)[[2]] <- list("arts", "science")
dimnames(numbers)[[1]] <- list("freshman", "sophomore")

numbers 

# convert table into a data frame 
as.data.frame.table(numbers)


frame <- as.data.frame.table(numbers)
names(frame) <- c("year", "discipline", "gender", "count")
frame


attach(frame)
model1 <- glm(count ~ year * discipline * gender, family = poisson)

model2 <- update(model1, ~. - year:discipline:gender)

anova(model1, model2, test = "Chi") # no significant difference 
detach(frame)
```

## Schoener's lizards: A complex contingency table 

Test if there are any separation across various factors and whether there are any interactions. 

```{r}

lizards <- read.table("lizards.txt", header = TRUE)
attach(lizards)
names(lizards)
# n is response variable


# saturated model 
model1 <- glm(n ~ sun * height * perch * time * species, family = poisson)

# remove the highest order interaction
model2 <- update(model1, ~.-sun:height:perch:time:species)
anova(model1, model2, test = "Chi")
# deviance is close to zero, no p value produed 

# remove a kind of four way interaction
model3 <- update(model2, ~.-sun:height:perch:species)
anova(model2, model3, test = "Chi")

# remove another four-way interaction
model4 <- update(model2, ~.-sun:height:time:species)
anova(model2, model4, test = "Chi")

model5 <- update(model2, ~.-sun:perch:time:species)
anova(model2, model5, test = "Chi")

model6 <- update(model2, ~.-height:perch:time:species)
anova(model2, model6, test = "Chi")

model7 <- step(model1, lower = ~sun*height*perch*time) # still two four way interactions left 
# lower argument prevent step from removing any interactions that don NOT 
# involve species, as they're essential 


# start from the lower model and all three way interactions
model8 <- glm(n ~ sun*height*perch*time + (species + sun + height + perch + time)^3, 
              family = poisson)

summary(model8)

# remove the four-way interaction
model9 <- step(model8, lower = ~sun*height*perch*time, trace = FALSE)

model10 <- update(model9, ~. -sun:height:species)
anova(model9, model10, test = "Chi")

# remove two-way interaction 
model11 <- update(model10, ~. -sun:species)
model12 <- update(model10, ~. -height:species)
model13 <- update(model10, ~. -perch:species)
model14 <- update(model10, ~. -time:species)
anova(model10, model11, test = "Chi")

anova(model10, model12, test = "Chi")

anova(model10, model13, test = "Chi")

anova(model10, model14, test = "Chi") # significant 


# a summary table 
ftable(tapply(n, list(species, sun, height, perch, time), sum))


# check if we need to keep all three levels for time of day 
tod <- factor(1 + (time == "Afternoon"))
model15 <- update(model10, ~.-species:time+species:tod)
anova(model10, model15, test = "Chi")
# two levels are ok 

detach(lizards)
```

## Plot methods fro contingency tables 

`assocplot` produce a Cohen-Friendly association plot indicating deviations from independence of rows and columns in a 2-dimensional contingency table.

`mosaicplot` plots a mosaic on the current graphics device.

`fourfoldplot` creates a fourfold display of a 2 by 2 by k contingency table on the current graphics device, allowing for the visual inspection of the association between two dichotomous variables in one or several populations (strata) 



```{r}
data(HairEyeColor)
(x <- margin.table(HairEyeColor, c(1, 2)) )
# margin.table computes the sum of table entries for a given index for a contingency table in array form.
assocplot(x, main = "Relation between hair and eye color")
# 1. the red bars show categories where fewer people were observed than expected 
# under the null hypothesis of independence of hair color and eye color. 
# 2. the black bard show the excess of people with black hair who have brown eyes etc

# same data plotted as mosaic plot 
mosaicplot(HairEyeColor, shade = TRUE)
# 1. indicates that there are significantly more blue eyed blond than expected in the case of independence
# 2. negative residuals are drawn in shades of red and with broken lines 
# 3. positive residuals are drawn in shades of blue with solid lines 

# admission policy of different departments 
data(UCBAdmissions)
head(UCBAdmissions)

str(UCBAdmissions)


x <- aperm(UCBAdmissions, c(2, 1, 3)) # transpose the x an y for each table 
# Transpose an array by permuting its dimensions and optionally resizing it.
x
UCBAdmissions

names(dimnames(x)) <- c("Sex", "Admit?", "Department")
ftable(x)

fourfoldplot(x, margin = 2)

# use gl to generate factor levels 
dept <- gl(6, 4)
dept
sex <- gl(2, 1, 24)
sex

admit <- gl(2, 2, 24)
admit

model1 <- glm(as.vector(x) ~ dept*sex*admit, family = poisson)
model2 <- update(model1, ~. -dept:sex:admit)
anova(model1, model2, test = "Chi")
# interaction significant 



# another way to do the same test as above 
# convert the three dim contingency table into a dataframe 
admissions <- as.data.frame(UCBAdmissions)
admissions



xtabs(Freq ~ Gender + Dept, admissions)
# xtabs creates a contingency table (optionally a sparse matrix) from cross-classifying factors, usually contained in a data frame, using a formula interface.

 
summary(xtabs(Freq ~ ., admissions))
 
str(xtabs(Freq ~ Admit + Dept + Gender, admissions))
xtabs(Freq ~ Admit + Dept + Gender, admissions)[, , 2]
 
females <- colSums(xtabs(Freq ~ Admit + Dept + Gender, admissions)[, ,2])
females

admitted.females <- xtabs(Freq ~ Admit + Dept + Gender, admissions)[, ,2][1, ]

(female.success <- admitted.females/females)
# the success rate varies a lot 
```


## Graphics for count data: Spine plots and spinograms 

The data for this section cannot be found from the book's websit. 

`spineplot` is  a special cases of mosaic plots, and can be seen as a generalization of stacked (or highlighted) bar plots. 

Analogously, `spinograms` are an extension of histograms.


In **spineplot(x, ...)**, x can be either categorical (then a spine plot is created) or numerical (then a spinogram is plotted). 


```{r}

# treatment and improvement of patients with rheumatoid arthritis
treatment <- factor(rep(c(1, 2), c(43, 41)), levels = c(1, 2),
                    labels = c("placebo", "treated"))
improved <- factor(rep(c(1, 2, 3, 1, 2, 3), c(29, 7, 7, 13, 7, 21)),
                   levels = c(1, 2, 3),
                   labels = c("none", "some", "marked"))

## (dependence on a categorical variable)
(spineplot(improved ~ treatment))


treatment <- as.numeric(treatment)
(spineplot(improved ~ treatment))

``` 
