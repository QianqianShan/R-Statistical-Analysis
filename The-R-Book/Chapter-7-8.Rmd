---
title: "Chapter 7 Mathematics | Chapter 8 Classical Tests"
author: "Qianqian Shan"
date: "May 26, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "therbookdata")
```


## Mathematical functions 

1. **Logarithmic functions** : $y = a\cdot ln(bx)$ and $y = a \cdot e^{bx}$. 

2. **Trigonometric functions** : $sin, cos, tan\cdots$.

3. **Power laws** : $y = a\cdot x^b$, different $b$ values can  result in different shapes.

4. **Polynomial functions** : useful for describing curves with humps, inflections, local maxima ...

5. **Gamma function** : $\Gamma(t) = \int_{0}^\infty x^{t-1}e^{-x} dx$ with plot 

```{r}
plot(seq(0.2, 4, 0.01), gamma(seq(0.2, 4, 0.01)), type = "l", 
     xlab = "", ylab = "")
```

6. **Asymptotic functions** : 

+ $y = \frac{ax}{1 + bx}$, extreme values at $x = 0$ or $x = \infty$. 

+ Asymptotic exponential, $y = a(1 - e^{-bx})$, with asymptotic value is $a$ when $x$ goes to $\infty$. 

7. **Sigmoid(S-shaped) functions** : 

+ two parameter logistic: $y = \frac{e^{a+bx}}{1 + e^{a+bx}}$, central to the generalized linear models. 

+ three parameter logistic: $y = \frac{a}{1 + be^{-cx}}$, allows $y$ to vary on any scale.

+ four parameter logistic: $y=a + \frac{b-a}{1+e^{c(d-x)}}$, it has a as left asympototes, b as right asympototes, c as scale, d as midpoint. 

+ **Gompertz growth model** : $y = ae^{be^{cx}}$, much used in demography and life insurance work, the shape depends on the signs of the parameters b and c. 

8. **Bi-exponential model** : $y=ae^{bx} + ce^{dx}$ 



**Summary for usefull transformations**

+ $log(y)$ against $x$ for **exponential** relationships. 

+ $log(y)$ against $log(x)$ for power functions. 

+ $exp(y)$ against $x$ for logarithmic relationships. 

+ $1/y$ agains $1/x$ for asympototic relationships. 

+ $log(\frac{p}{1-p})$ against $x$ for proportional data. 

+ $\sqrt y$ to stabilize the variance for count data. 

+ $arcsin(y)$ to stablize hte variance of percentage data. 

```{r}

``` 

## 7.3 Probability functions , ignored 

## 7.4 Discrete probability distributions , ignored 

## 7.5 Matrix algebra 

+ **Determinant** : More details on linear algebra books. 1. If any row or column of a determinant is multiplied by a scaler $\lambda$, then the value of the determinant is multiplied by $\lambda$. 2. If all the elements of a row or a column are zero then the determinant $|A|=0$.  If $det A \ne 0$ then the rows and columns of $A$ must be linearly independent, more details on contrasts on Chapter 9. 

+ **Inverse of a matrix** : $A^{-1} = \frac{adj A}{|A|}$ where $adj A$ is the adjoint matrix of $A$ with $A_{ij} = (-1)^{i+j}M_{ij}$. More details [here](https://en.wikipedia.org/wiki/Invertible_matrix).

$(AB)^{-1} = B^{-1}A^{-1}, \\  (A^{-1})^{\prime} = (A^\prime)^{-1},\\ (A^{-1})^{-1}=A, \\ |A| = \frac{1}{|A^{-1}|}$, `ginv` from `MASS` can be used to find the inverse. 

+ **Eigenvalues and eigenvectors** : check [wiki](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) for more details. `eigen` is useful. 

## Solving systems of linear equations using matrices 

`solve` is used. 

```{r}
A <- matrix(c(3, 4, 1, 2), nrow = 2)
A 
kv <- matrix(c(12, 8), nrow = 2)
kv

# solve the equations 
solve(A, kv) # x and y values 
```

## Calculus 

+ `D` and `integrate` for derivative and integration 

+ differential equations by `deSolve` 


```{r}

# use D
D(expression(x^2), "x")

# use deriv 
## formula argument :
dx2x <- deriv(~ x^2, "x") ; dx2x

mode(dx2x)
# evaluate the drivative at specific values 
x <- -1:2
eval(dx2x)

## Something 'tougher':
trig.exp <- expression(sin(cos(x + y^2)))
( D.sc <- D(trig.exp, "x") )
all.equal(D(trig.exp[[1]], "x"), D.sc)

( dxy <- deriv(trig.exp, c("x", "y")) )
y <- 1
eval(dxy)
eval(D.sc)


## Higher derivatives
deriv3(y ~ b0 + b1 * 2^(-x/th), c("b0", "b1", "th"),
     c("b0", "b1", "th", "x") )

## Higher derivatives:
DD <- function(expr, name, order = 1) {
   if(order < 1) stop("'order' must be >= 1")
   if(order == 1) D(expr, name)
   else DD(D(expr, name), name, order - 1)
}
DD(expression(sin(x^2)), "x", 3)



# integrals 
integrate(dnorm, 0, Inf)

integrate(dnorm, -Inf, Inf)

# integrate a self-defined function 
integrate(function(x) rep(2, length(x)), 0, 1)

integrand <- function(x) {1/((x + 1) *sqrt(x))}
integrate(integrand, 0, Inf)

integrand(seq(0.1, 10, 0.1))
curve(integrand, 0.1, 10) # area under curve is pi 


# install.packages("deSolve")
library(deSolve)

# 1. define a function to contain the equations 

phmodel <- function(t, state, parameters){
  with(as.list(c(state, parameters)), {
    dv <- r*v*(k-v)/k - b*v*n # equation one 
    dn <- c*v*n - d*n  # equation two 
    result <- c(dv, dn)
    list(result)
  })
}


# 2. generate a times series over which to solve the equations and set parameters
times <- seq(0, 1000, length = 1001)
parameters <- c(r = 0.4, k = 1000, b = 0.02, c = 0.01, d = 0.3)

# set initial values for v and n 
initial <- c(v = 50, n = 10)

# use ode to create a matrix with the time series of v and n 
# ode : Solves a system of ordinary differential equations; 
#       a wrapper around the implemented ODE solvers

output <- ode(y = initial, time = times, func = phmodel, parms = parameters)
head(output)
plot(output[, 1], output[, 2],
     ylim = c(0, 60), type = "n", ylab = "values",
     xlab = "time", main = "dN/dt and dV/dt are both zeros with corresponding stable n and v values")
lines(output[, 1], output[, 2], col = 3)
lines(output[, 1], output[, 3], col = 4)



# alternative way is to show the phase plane 
plot(output[, 2], output[, 3], 
     ylim = c(0, 50), xlim = c(0, 70), type = "n",
     ylab = "n", xlab = "v")
lines(output[, 2], output[, 3], col = 4)
```


# Chapter 8 Classical Tests 

## Single samples 

### Summary 

* `plot(y)` for index plot 

* `hist` for histogram 

* `boxplot` 

* `summary` 

* `fivenum(y)` for Tukey's five number 

* `outliers` rule of thumb: an **outlier** is a value that is more than 1.5 times the interquartile range above the third quartile or below the first quartile. 



### Test for normality 

* `qqnorm` and `qqline` , `qqplot`(produce QQ plot of **two** datasets).

* `shapiro.test` for testing whether the data in a vector com from a normal distribution. 
Note again that the **p value** is an estimate of the probability that a particular result or a more extreme result than the observed result could have been observed. 

However, p values only reflect the effect sizes, while sample sizes are equally important. 

```{r}
y <- rt(200, df = 5)
qqnorm(y); qqline(y, col = 2)
qqplot(y, rt(300, df = 5))

shapiro.test(y) # reject the null that the sample data are normally distributed


# example 

light <- read.table("light.txt", header = TRUE)
attach(light)
names(light)
length(speed)  # only 20 samples 
hist(speed, main = "", col = 3)  # not normal 

# as it's not normal, use wilcoxon singed-rank test to test if the speed is significantly different from a value 
wilcox.test(speed, mu = 990) # reject the null 
``` 

## Bootstrap in hypothesis testing 

```{r}
# test if the mean is as big as 990
mean(speed)
a <- numeric(10000)
for(i in 1:10000) a[i] <- mean(sample(speed, replace = TRUE))
hist(a, main = "", col = "blue") 
detach(light)
``` 

## Skew and Kurtosis 

**Skew(ness)** is the dimensionless version of the third moment about the mean, it measures the extent to which a distribution has long, drawn-out tails on one side or the other: 

$skew = \gamma_1 = \frac{m_3}{s_3}$, where $m_3 = \frac{\sum (y - \overline{y})^3}{n}$ and $s_3 = (sd(y))^3 = (\sqrt{s^2})^3$.  Negative values means skew to the left. 

+ Approximate standard error of skew $se_{\gamma_1} = \sqrt{\frac{6}{n}}$.

**Kurtosis** is the dimensionless version of the fourth moment about the mean, it meansures the non-normality that has to do with the peakyness, or flat-toppedness of a distribution. 

* A more flat-topped distribution is **platykurtic**, a more pointy distribution is **leptokurtic**. Plots shown below. 

$kurtosis = \gamma_2 = \frac{m_4}{s_4} - 3$, where $m_4 = \frac{\sum(y - \overline{y})^4}{n}$, $s_4 = (var(y))^2 = s^4$ and -3 is included as the normal distribution has kurtosis 3.

+ Approximate standard error of kurtosis $se_{\gamma_2} = \sqrt{\frac{24}{n}}$.

```{r}

# positive and negative skew 
par(mfrow = c(1, 2))
x <- seq(0, 4, 0.01)
plot(x, dgamma(x, 2, 2), type = "l", ylab = "f(x)", 
     xlab = "x", col = "4")
text(2.7,0.5,"positive skew")
plot(4-x, dgamma(x,2,2), type = "l",ylab = "f(x)",xlab = "x",col = 4)
text(1.3, 0.5, "negative skew")
par(mfrow = c(1, 1))

# calculate the skew 
skew <- function(x){
m3 <- sum((x-mean(x))^3)/length(x); s3 <- sqrt(var(x))^3
m3/s3
}

# find the skew value and perform a t test to test if the skew is significantly different from zero 
data <- read.table("skewdata.txt",header=T) 
attach(data)
names(data)
skew(values) # skew value 

# t test 
t.statistic <- skew(values)/sqrt(6/length(values))
1 - pt(t.statistic, length(values) - 1) # reject the null (normality)



# kurtosis 
# difference between leptokurtsis and platykurtosis 
par(mfrow = c(1, 2))
plot(-200:200, dcauchy(-200:200,0,10), type="l", ylab="f(x)", xlab="", yaxt="n",
            xaxt="n", main="leptokurtosis", col="red")
xv <- seq(-2, 2, 0.01)
plot(xv, exp(-abs(xv)^6), type="l", ylab="f(x)", xlab="", yaxt="n",
            xaxt="n", main="platykurtosis", col="red")
par(mfrow = c(1 , 1))


# function to calculate the kurtosis 
kurtosis <- function(x) {
m4 <- sum((x - mean(x))^4)/length(x) 
s4 <- var(x)^2
m4/s4 - 3 }

kurtosis(values)
detach(data)
``` 


## Two samples 

* Fisher's F tset by `var.test` for comparing two variances; Fligner-Killeen test and Bartlett test afor multiple samples by `fligner.test` and `bartlett.test`

* Student's t test by `t.test` for comparing two sample means with **normal** errors 

* Wilcoxon's rank test by `wilcox.test` for comparing two sample means with **non-normal** errors : **non-parametric**

* Binomial test by `prop.test` for comparing two proportions 

* Pearson's or Spearman's rank correlations by `cor.test` for correlations of two variables 

* Chisq square test by `chisq.test` or Fisher's exact test by `fisher.test` for testing the independence of two variables in a contingency table. 

```{r}
# compare two variances
data <- read.table("f.test.data.txt", header = TRUE)
attach(data)
names(data)
var.test(gardenB, gardenC) # significant variance 

detach(data)


# more than two samples 

ozone <- read.table("gardens.txt",header=T)
attach(ozone)
names(ozone) # 10 by 3 dataframe 
y <- c(gardenA,gardenB,gardenC)
garden <- factor(rep(c("A","B","C"),c(10,10,10)))
var.test(gardenB, gardenC) 

bartlett.test(y ~ garden)

fligner.test(y ~ garden)
detach(ozone)
# fligner test is different with the other two tests 
# because Fisher and Bartlett are sensitive to outliers while Fligner is not. 




# student's t test 
t.test.data <- read.table("t.test.data.txt",header=T) 
attach(t.test.data)
par(mfrow=c(1,1))
names(t.test.data)

ozone <- c(gardenA, gardenB)
label <- factor(c(rep("A",10), rep("B",10)))
boxplot(ozone ~ label, notch=T, xlab="Garden", ylab="Ozone")

# carry out the t test in the long hand 
s2A <- var(gardenA)
s2B <- var(gardenB)

# t statistic 
t.statistic <- (mean(gardenA) - mean(gardenB))/sqrt(s2A/10+s2B/10)

# p value for two tailed test  
2*pt(t.statistic, length(gardenA) + length(gardenB) - 2)


# an easier way to do t test 
t.test(gardenA, gardenB)



# wilcoxon rank-sum test  
ozone <- c(gardenA, gardenB)
ozone

label <- c(rep("A", 10), rep("B", 10))
label

combined.ranks <- rank(ozone)
combined.ranks

tapply(combined.ranks, label, sum)

wilcox.test(gardenA,gardenB) # reject the null (the mean ozone between the two are the same)


#  test on paired samples 
streams <- read.table("streams.txt",header=T) 
attach(streams)
names(streams)

# a t test treating as unpaired data 
t.test(down, up)

# paired t test for "true mean equals to 0 "
t.test(down, up, paired = TRUE)

# another way for paired test 
difference <- up - down
t.test(difference)
detach(streams)

# the sign test 
# 

sign.test <- function(x, y){
  if(length(x) != length(y)) stop("The two variables must be the same length") 
  d <- x - y
  binom.test(sum(d > 0), length(d))
}
sign.test(gardenA,gardenB)
# sign test is non parametric, with other things equal, the parametric test will be more powerful. 
detach(t.test.data)


# proportion test is useful when sample sizes are not equal 
prop.test(c(4,196),c(40,3270))

``` 

## 8.8 Chi-squared contigency tables , ignored 


## Correlation and covariance 


+ **Correlation** : $r = \frac{cov(x, y)}{\sqrt{s_x^2 \cdot s_y^2}}$.

+ **Partial correlation** for data with more than two variables: the correlation of x and y given the third variable z constant, $r_{xy.z} = \frac{r_{xy} - r_{xz}r_{yz}}{\sqrt{(1-r_{xz}^2)(1-r_{yz}^2)}}$.

```{r}
pollute <- read.table("Pollute.txt", header = TRUE)
attach(pollute)

# correlation matrix 
cor(pollute)

# or do the correlation  long hand 
varp <- var(Pollution)
varw <- var(Wet.days)
varc <- var(Pollution, Wet.days)

# the correltion is 
varc/sqrt(varp*varw)

# correlation between two vectors 
cor(Pollution, Wet.days)



# correlation test 
cor.test(Pollution, Wet.days)

detach(pollute)



## scale dependent correlations 
productivity <- read.table("productivity.txt", header = TRUE)
rm(x)
rm(y)
attach(productivity)
names(productivity)

# the overall relationship between x and y 
plot(x, y, pch = 21)

# correlation test 
cor.test(x, y) # significant positive correlation 

# correlation for each region(f)
coplot(y~x|f, panel = panel.smooth, main = "") # totally different 
detach(productivity)

``` 


## Kolmogorov-Smirnov test 

It works on **cumulative distribution functions**. 

* Are the two sample distributions the same? 

* Does a particular sample distribution arise from a particular hypothesized distribution? 

```{r}
x <- rnorm(50)
y <- runif(30)
# Do x and y come from the same distribution?
# Perform a one- or two-sample Kolmogorov-Smirnov test.

ks.test(x, y) # marginally not significant 

# Does x come from a shifted gamma distribution with shape 3 and rate 2?
ks.test(x+2, "pgamma", 3, 2) # two-sided, exact
ks.test(x+2, "pgamma", 3, 2, exact = FALSE)
ks.test(x+2, "pgamma", 3, 2, alternative = "gr")

# test if x is stochastically larger than x2
x2 <- rnorm(50, -1)
plot(ecdf(x), xlim = range(c(x, x2)))
plot(ecdf(x2), add = TRUE, lty = "dashed")
t.test(x, x2, alternative = "g")
wilcox.test(x, x2, alternative = "g")
ks.test(x, x2, alternative = "l")

rm(x)
rm(y)
``` 


## Power analysis 

The power of a test is the probability of rejecting the null hypothesis when it is false. 

* Type I error: reject the null when it's correct. 

* Type II error: accept the null when it's false. 

* `power.t.test` for power calculations of one- and two-sample t tests;

* `power.prop.test` for power calculation of two sample test for proportions; 

* `power.anova.test` for power calculations of balanced one-way ANOVA tests. 

For example, for a t test for two sample means, the t statistic is $t=\frac{d}{\sqrt{2\frac{s^2}{n}}}$, or equivalently, $n = \frac{2s^2 t^2}{d^2}$.


```{r}
# detect a difference of 10% when mean is 20, i.e., delta = 20, power is 80% and sd 3.5 
power.t.test(delta = 2, sd = 3.5, power = 0.8)

# the size of difference for two samples (n =15 for each)

power.t.test(n = 15, sd = 3.5, power = 0.8)

``` 

## Bootstrap 

* Obtain confidence intervals for the mean vector 

```{r}
data <- read.table("skewdata.txt", header = TRUE)
attach(data)
names(data)
ms <- numeric(10000)
for(i in 1:10000){
  ms[i] <- mean(sample(values, replace = T))
}

quantile(ms, c(0.025, 0.975)) # the CI with skewed data is also skewed 


# use "boot" package to do the same thing 
library(boot)

# boot Generate R bootstrap replicates of a statistic applied to data.

# function which is the statistic of interest 
mymean <- function(values, i) mean(values[i])

myboot <- boot(values, mymean, R = 10000)
myboot
names(myboot)
hist(myboot$t, main = "", xlab = "replicates")
quantile(myboot$t, c(0.025, 0.975))
detach(data)
``` 


