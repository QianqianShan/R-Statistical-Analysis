---
title: "Chapter 13 Generalized Linear Models"
author: "Qianqian Shan"
date: "June 4, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "therbookdata")
```
**More details will be added on generalized linear model**, see lecture notes <https://dnett.github.io/S510/notes.html> for more details. 

##  Error structure 

* Poisson errors for count data 

* binomial errors for data on proportions 

* gamma errors for data showing a constant coefficient of variation 

* exponential errors for data on time to death(survival analysis)


Error structure is defined by means of `family` as part of the model formula. 

**Canonical link functions**(default link functions): 

Error | Cannonical link 
-----------|------------------
normal|identiy 
poisson | log 
binomial | logit 
Gamma | reciprocal 


**Quasi-likelihood** ï¼š If the variability of our response is greater than we should expect based on our estimates of the mean, we say that there is overdispersion.

```{r}
data<-read.table("timber.txt",header = TRUE)
attach(data)
head(data)

# fit the model with quasilikelihood but with different lind function 
model1 <- glm(volume ~ girth + height, family = quasi(link = power(1/3))) # cubic root link 
model2 <- glm(volume ~ girth + height, family = quasi(link = log)) # log link 

# compare 
anova(model1, model2)
# dfs are the same but model 2 has smaller residual deviance and thus better 

# assess the constancy of variance and normality for both models 
par(mfrow = c(2, 2))
plot(model1)
plot(model2)
par(mfrow = c(1, 1))
# model 2 is better 

detach(data)

```


## Generalized additive models 

The shape of the relationship between y and a continuous variable x is specified by non-parametric smoothers, not specified by some explicit functional form.


## Offset 

A component of a linear predictor that is known in advance from theory, mechanistic model, and thus requires no parameter to be estimated from the data, then `offset` is useful, see examples below. 

Example: It's already known that $v = \frac{g^2}{4\pi}h$, and $log(v) = log(\frac{1}{4\pi} + 2log(g) + log(h)$, that is to say, the coefficients of the intercept, $log(g)$ and $log(h)$ are already known. 


```{r}
# offset 
data <- read.delim("timber.txt") 
attach(data)
names(data)

# rescale the girth to be consistent with other vairables 
girth <- girth/100

# fit model without offset 
model1 <- glm(log(volume) ~ log(girth) + log(height))
summary(model1)
# coefficients close to theoretical ones but not the same 


# specify the slope for log(h) to be 1 
model2 <- glm(log(volume) ~ log(girth) + offset(log(height)))
summary(model2)
AIC(model2)

# specify the slope of both variables
model3 <- glm(log(volume) ~ 1 + offset(log(height) + 2 * log(girth)) )
summary(model3)


# theoretical intercept 
log(1/(4 * pi))

# 
model4 <- glm(log(volume) ~ offset(log(1/(4 * pi)) + log(height) + 2 * log(girth)) - 1)
summary(model4)



models <- list(model1, model2, model3, model4)
unlist(lapply(models, AIC)) # model4 is the best
``` 

## Overdispersion 

Overdispersion can be a problem when working with Poisson or binomial errors, and tends to occur because you have not measured one or more of the factors that turn out to be important. 


## Bootstrap a GLM 

```{r}

library(boot)

# statistic function in full 
# sample all data with replacement 
model.boot <- function(data, indices){
sub.data <- data[indices, ]
model <- glm(log(volume) ~ log(girth) + log(height), data = sub.data)
coef(model) }

glim.boot <- boot(trees, model.boot, R = 2000)
glim.boot



# sample the residual with homemade function 
model <- glm(log(volume) ~ log(girth) + log(height))
yhat <- fitted(model)
residuals <- log(volume) - yhat

coefs <- numeric(6000)
coefs <- matrix(coefs,nrow=2000)

# shuffle the residuals 2000 times to get new y vectors 
for (i in 1:2000){
y <- yhat + sample(residuals)
boot.model <- glm(y ~ log(girth) + log(height)) 
coefs[i, ] <- coef(boot.model)
}

apply(coefs, 2, mean)

apply(coefs, 2, sd)


# shuffle the residuals using built in function in boot 
model <- glm(log(volume) ~ log(girth) + log(height))
yhat <- fitted(model)
resids <- resid(model)

# original residuals and covariates 
res.data <- data.frame(resids, girth, height)

# statistic function 
bf <- function(res.data, i) {
y <- yhat + res.data[i, 1] # i is a set of indexs given by the boot function 
nd <- data.frame(y, girth, height) 
model <- glm(y ~ log(girth) + log(height), data = nd)
coef(model) 
}

boot(res.data, bf, R = 2000, sim = "permutation")


perms <-  boot(res.data, bf, R = 2000, sim = "permutation")
boot.ci(perms, index=1)

boot.ci(perms,index=2)

boot.ci(perms, index = 3)

detach(data)
``` 


## Binomial GLM with ordered cateforical variables 



```{r}
# built-in data esoph
str(esoph)
model1 <- glm(cbind(ncases, ncontrols) ~ agegp + alcgp * tobgp, family = binomial, data = esoph)
summary(model1) # no overdispersion 

# remove interaction term 
model2<-glm(cbind(ncases, ncontrols) ~ agegp + alcgp + tobgp,family = binomial, data = esoph)
anova(model1, model2)

qchisq(.95, 9) # justified

# look at data as proportions 
attach(esoph)
p <- ncases/(ncases + ncontrols)


# plot against explanatory variables 
par(mfrow=c(2,2))
plot(p ~ alcgp, col = "red")
plot(p ~ tobgp, col = "blue")
plot(p ~ agegp, col = "green")
par(mfrow = c(1, 1))


# combine similar levels 
tob2 <- tobgp
levels(tob2)[2:3] <- "10-30"
levels(tob2)

age2 <- agegp
levels(age2)[4:6] <- "55+"
levels(age2)[1:2] <- "under45"
levels(age2)


model3 <- glm(cbind(ncases, ncontrols) ~ age2 * alcgp * tob2, family = binomial, data = esoph)
model4 <- step(model3)

model5 <- update(model4, ~ .-age2:alcgp)
anova(model4, model5, test = "Chi")

detach(esoph)
``` 



