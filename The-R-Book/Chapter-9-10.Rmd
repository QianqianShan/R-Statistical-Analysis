---
title: "Chapter 9 Statistical Modeling | Chapter 10 Regression"
author: "Qianqian Shan"
date: "May 30, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "therbookdata")
```

**Appropriate statistical mothods**


**Explanatory variables** | **Method**
  ------------- | -------------
  All continuous    | Regression
  All categorical   | ANOVA 
  Both continuous and categorical | ANCOVA(covariance)
  

**Response Variables Type** | **Method** 
----------------------------|---------------
Continous | Normal regresssion, ANOVA, ANCOVA
proportion | Logistic regression 
Count | Log-linear models 
Binary | Binary logistic regression 
Time at death | Survival analysis 

 

**A model should be as simple as simple, but no simpler.** 

**Types of statistical models**: 

**Model** | **Fit** | **Degree of Freedom** | **Explanatory Power** | **Interpretation**
----------|---------|-----------------------|----------------------|--------------------
Saturated model | Perfect | None | None | One parameter for every data point 
Maximal model |  |$n-p-1$ | Depends | Contatins all p factors, interactions, covariates etc. 
Minimal adequate model | less than maximal but not significant | $n - p^\prime - 1$| $r^2 = SSR/SSY$| Simplified model wiht $1 \le p^\prime \le p$ parameters
Null model | None | $n-1$ | None | Just one parameter, i.e., the overall mean $\overline{y}$



**Formulae in R** : 

**Model** | **Formula** | **Comments**
------------------------|-------------------------|----------------------------------
Null | $y \sim 1$ | 1 for intercept 
Regression | $y \sim x$ | x is continuous 
Regression w/o intercept | $y \sim x -1$ | 
One-way ANOVA | $y \sim sex$ | Categorical variable
Two-way ANOVA | $y \sim sex + genotype$ | Two categorical variables 
Factorial ANOVA| $y \sim N*P*K$ | Factors with all their interactions 
Three-way ANOVA | $y \sim N*P*K - N:P:K$ | Same as above except that no three-way interaction
Analysis of Covariance | $y \sim x  + sex$ | sex categorical, x continuous, common slope for x with two intercepts for sex 
Analysis of Covariance | $y \sim x*sex$ |Two slopes and two intercepts 
Nested ANOVA | $y \sim a/b/c$ | Factor c nested within factor b within factor a
Split-plot ANOVA | $y\sim a*b*c + Error(a/b/c)$ | Factorial experiment but with three plot sizes and three different error variances 
Multiple Regression | $y\sim x * z$ | Fit two continuous variables together with interactions
Multiple Regression | $y \sim x + I(x \hat{} 2) + z + I(z \hat{} 2)$| Quadratic term for each 
Multiple Regression |$y \sim poly(x, 2) + z$ | Quadratic polynomial for x and linear for z 
Multiple Regression | $y \sim (x + z + 2)\hat{}2$ | Fit three variables with interactions up to two-way 
Non-parametric Model | $y \sim s(x) + s(z)$ | A function of smoothed x and z in a generalized additive model 
Transformed Response and Variables | $log(y) \sim I(1/x) + sqrt(z)$ | All variables transformed 


**Note**: we need to use I() if want to use a transformed variable in the formula. 

## Model formulae in R 
  
```{r}

# create formula objects using "collapse" and "paste"
xnames <- paste("x", 1:25, sep = "")
model.formula <- as.formula(paste("y ~", paste(xnames, collapse = "+")))
model.formula
``` 

## `update` function in model simplication 

With `model` as the previously speicified model, the following statement removes the interaction term `A:B`:
$model2 <- update(model, ~ . -A:B)$.


## Box-Cox transformations 

A simple empirical solution for optimal transformation of the response variables. 

**Idea**: find the power transformation $\lambda$, that maximizes the likelihood when a speicified set of explanatory variables is fitted to $\frac{y^\lambda - 1}{\lambda}$, while the transformation is $log(y)$ when $\lambda = 0$.


```{r}
data <- read.delim("timber.txt")
attach(data)
names(data)
library(MASS)

# boxcox : Computes and optionally plots profile log-likelihoods for the parameter of the Box-Cox power transformation.
par(mfrow = c(1, 2))
boxcox(volume ~ log(girth) + log(height))

# zoom the area with maximal likelihood 
boxcox(volume ~ log(girth) + log(height), lambda = seq(-0.5, 0.5, 0.01))
detach(data)
par(mfrow = c(1, 1))

``` 


## Model checking 

1. Residuals against -

* fitted values for heteroscedasticity (standardized residuals against fitted values)

* explanatory variables for evidence of curvature 

* the sequence of data collection for temporal correlation 

* standard normal deviates for non normality of errors. 

2. Influential data points 

3. Overdispertion 

4. Depends 



```{r}

# model check function: 
# plot residuals vs fitted values and plot qqplot again normal data 

mcheck <- function (obj,...){ 
      rs <- obj$resid
      fv <- obj$fitted
      par(mfrow = c(1,2))
      plot(fv, rs, xlab="Fitted values", ylab="Residuals", pch=16, col="red")
      abline(h=0, lty=2)
      qqnorm(rs, xlab = "Normal scores", ylab="Ordered residuals", main="", pch=16) 
      qqline(rs, lty=2, col = "green")
      par(mfrow = c(1,1))
      invisible(NULL) }

x <- 0:30 
e <- rnorm(31, 0, 1)
y <- 10 + x + e 
mn <- lm(y ~ x)
mcheck(mn)
rm(x)
rm(y)
rm(e)
``` 


## Influence 

```{r}

x <- c(2,3,3,3,4)
y <- c(2,3,2,1,2)
par(mfrow=c(1,2))
plot(x, y, xlim=c(0,8), ylim=c(0,8))

# add an outlier 
x1 <- c(x, 7)
y1 <- c(y, 6)

plot(x1, y1, xlim = c(0,8), ylim = c(0,8))
abline(lm(y1~x1), col = "blue")
par(mfrow = c(1, 1))

# fit the regression 
reg <- lm(y1~x1)
summary(reg)

# measure the influence of every point 
influence.measures(reg)

influence.measures(reg)$is.inf

lm.influence(reg)

# model withouth the outlier 
summary.aov(lm(y1[-6]~x1[-6]))



``` 


## Summary of statistical models in R 

**Models in R**|**Description** 
------------|-----------------------------------------------------------------------------------------------------
lm | linear model with **normal** errors and **constant** variance; generally used for regression for continuous variables.
aov | fit analysis of variance with **normal** errors, **constant** variance and **identity** link; generally for categorical variables or ANCOVA with a mix of categorical and continuous variables. 
glm | generalized linear models to data using categorical or continuous variables , by specifying one of a family of **error structure** and a particular **link function**. 
gam | generalized additive models to data with a family of errosr structures in which the continuous varibles can be fitted as **arbitrary smoothed functions** using **non-parametric** smoothers rather than the specific parameter functions. 
lme, lmer | fit linear mixed-effects models with specified mixtures of fixed effects and random effects, allow for the **specification of correlation structure among explanatory variables and autocorrelation of the response variable**. 
nls | non-linear regression model via **least squares**. 
nlme|non-linear mixed-effects model where parameters of the non-linear function are assumed to be random effects; allows for **specification of correlation structure among explanatory variables and autocorrelation of the response variable**.
loess| local regression model using **non-parametric** techniques to produce a smoothed model surface. 
tree, rpart | fit a regression tree/classification tree using binary recursive partitioning.  


## Optional arguments in model-fitting functions 

1. `subset`, fit the model to a subset of the data. 

2. `weights`, fit the model with data points of unequal weights.  

3. `offset`, fit **generalized linear models** to specify part of the variation in the response.  

4. `na.action`, deal with  missing values:

* `na.action = na.omit` to leave out any row which has at least one variable missing 

* `na.action = na.fail` to fail the fitting process 

* `na.action = NULL` to carry out regression with time series data taht include missing values, so the residuals and fitted values are time series as well. 


```{r}

# the use of "subset"
data <- read.table("ipomopsis.txt", header = TRUE)
attach(data)
names(data)
model <- lm(Fruit ~ Root, subset = (Grazing == "Grazed"))
# summary(model)

# weights are equal by default 
model <- lm(Fruit ~ Grazing, weights = Root) # fit by weighted least squares 
detach(data)
``` 

## Akaike's information criterion (AIC) 
Also known as penalized log likelihood. 

$AIC = -2 \times loglikelihood + 2(p+1)$, where $p$ is the number of parameters in the model, 1 is added for the estimated variance.

```{r}
data <- read.table("regression.txt", header = TRUE)
attach(data)
names(data)

model <- lm(growth ~ tannin)

# calculate the log likelihood by hand 
n <- length(growth)
sse <- sum((growth - fitted(model))^2)
s2 <- sse/ (n - 2)
s <- sqrt(s2)

# the log likelihood 
loglike <- -(n/2) * log(2*pi) - n*log(s) - sse/(2*s2)
loglike
# AIC 
-2*loglike + 2*(2 + 1)


# use an easier to calculate likelihood and AIC 
logLik(model)
AIC(model)

detach(data)

data <- read.table("ipomopsis.txt", header = TRUE)
attach(data)
# AIC as a measure of the fit of a model 
model.1 <- lm(Fruit ~ Grazing * Root)
model.2 <- lm(Fruit ~ Grazing + Root)
AIC(model.1, model.2)

# compare multiple models using AIC 
model.3 <- lm(Fruit ~ Grazing * Root + I(Root^2))

models <- list(model.1, model.2, model.3)
aic <- unlist(lapply(models, AIC))
aic
detach(data)
``` 

## Leverage 

Measures of levearage for a given data point $y$ are proportional to $(x - \overline{x})^2$. 

One common measure is : 

$h_i = \frac{1}{n} + \frac{(x_i - \overline{x})^2}{\sum(x_i - \overline{x})^2}$  and the rule of thumb that a point is highly influential is if $h_i > \frac{2p}{n}$, where $p$ is the number of parameters and n is the sample size. 

```{r}
x <- c(2, 3, 3, 3, 4, 7)
leverage <- function(x) {1/length(x) + (x - mean(x))^2 / sum((x-mean(x))^2)}

plot(leverage(x), type = "h", ylim=c(0, 1), col = "blue")
abline(h = (2 * 2)/length(x), lty = 2, col = 3) # the sixth data point is influential 
``` 


## Model checking in R using `plot(model)`

```{r}
decay <- read.table("Decay.txt", header = TRUE)
attach(decay)
names(decay)
model <- lm(amount ~ time)
par(mfrow = c(2, 2))
plot(model) # the first two plots are important 
# the third is a postive valued version of the first graph 
# the fourth plot is standardized residual vs. levearge tother with Cook's distance, 
# which is a combination of leverage and residuals in a single measure. 
par(mfrow = c(1, 1))
detach(decay)

``` 


## Extracting information from model objects 

1. by name, e.g. , coef(model)

2. with list subscripts, e.g., summary(model)[[3]]

3. using `$` , e.g., model$resid 

```{r}
# by name 
data <- read.table("regression.txt",header=T) 
attach(data)
names(data)
model <- lm(growth~tannin)
summary(model)
coef(model)
fitted(model)
resid(model)
vcov(model) # variance covariance matrix 


# by list subscripts 
summary.aov(model)
str(summary.aov(model)) # list of 1 
summary.aov(model)[[1]][1]



as.numeric(unlist(summary.aov(model)[[1]][4]))[1]




# using lists with models 
x <- 0:100
y <- 17 + 0.2 * x + 3 * rnorm(101)


model0 <- lm(y ~ 1)
model1 <- lm(y ~ x)
model2 <- lm(y ~ x + I(x^2))

models <- list(model0, model1, model2)
lapply(models, coef) # check coefs of all models 

# get a vector as output, all three intercepts 
as.vector(unlist(lapply(models,coef)))[c(1,2,4)]

lapply(models,AIC) # AIC 

rm(x)
rm(y)
detach(data)
``` 


## Contrasts 


**Rules for constructing coefficients**: 
1.  Treatment to be lumped together get the same sign;

2. Groups of means to be contrasted get opposite sign; 

3. Factor levels to be excluded get a contrast coefficient of 0;

4. The coeffcients add up to 0. 


**Contrasts sum of squares**: $SSC = \frac{(\sum\frac{c_i T_i}{n_i})^2}{\sum\frac{c_i^2}{n_i}}$, where $T_i$ is the total of the y values within factor level i. 

```{r}
# example of contrast 
comp <- read.table("competition.txt",header = TRUE) 
attach(comp)
names(comp)

model1 <- aov(biomass ~ clipping)
summary(model1)

summary.lm(model1) # summary for lm method 

levels(clipping)


# A priori contrasts 
# use specified priori test 
contrasts(clipping) <- cbind(c(4,-1,-1,-1,-1),
                             c(0,1,1,-1,-1),
                             c(0,0,0,1,-1),
                             c(0,1,-1,0,0))


clipping

# refit the model 
model2 <- aov(biomass ~ clipping)
summary.lm(model2) # coefs are different, se and df are the same 

# first coef is the mean of biamass 
mean(biomass) 

# the means for different factor levels 
tapply(biomass, clipping, mean)


# the first contrast
c1 <- factor(1 + (clipping != "control"))
tapply(biomass, c1, mean)

# the second estimate in the summary is the difference between the overall mean and 
# the mean of the four other treatments 
mean(biomass) - tapply(biomass,c1,mean)[2]


# third contrast for the corresponding group means comparison 
c2 <- factor(2*(clipping == "n25") + 2*(clipping == "n50")+
                        (clipping == "r10") + (clipping == "r5"))

tapply(biomass, c2, mean)
(tapply(biomass, c2, mean)[3]- tapply(biomass, c2, mean)[2])/2

# rm(biomass)
rm(clipping)
detach(comp)
``` 

## Comparison of three kinds of contrasts 

* Treatment contrasts: the default contrast

* Helmert contrasts: default in S-PLUS

* Sum contrasts: see below 
```{r}

# treatment contrasts 
options(contrasts = c("contr.treatment", "contr.poly"))
comp <- read.table("competition.txt",header = TRUE) 
attach(comp)
contrasts(clipping) # NOT  orthogonal

output.treatment <- lm(biomass ~ clipping)
summary(output.treatment)


# Helmert contrasts 
options(contrasts = c("contr.helmert", "contr.poly"))
contrasts(clipping)

output.helmert <- lm(biomass~clipping)
summary(output.helmert)


# sum contrast 
options(contrasts=c("contr.sum","contr.poly"))
contrasts(clipping)
output.sum <- lm(biomass ~ clipping)
summary(output.sum)
tapply(biomass,clipping,mean) - 561.8 # the estimates 
detach(comp)
# reset the default 
options(contrasts = c("contr.treatment", "contr.poly"))

``` 


## Aliasing

Occurs when there is no information available on which to base an estimate of a parameter value. 

1. Intrinsic aliasing occurs when it's due to the structure of the model; for example, two variables are perfectly correlated, then including both into a model will result in one zero parameter estimate. 

2. Extrinsic aliasing occurs when it's due to the nature of the data; for example, a certain combination of the factors have zero observations accidentally, then this particular combination will contribute do data to the response variable and then cannot be estiamted. 


## Polynomial contrasts 

**Orthogonal polynomial contrasts** for a factor with four levels:

term | $x_1$ | $x_2$ | $x_3$ |$x_4$ 
-------------|-----------|------------|--------------|-------------------------
linear| -3 | -1 | 1| 3 
quadratic | 1 | -1 | -1 | 1
cubic | -1 | 3 | -3 | 1

```{r}
data <- read.table("poly.txt", header = TRUE)
attach(data)
names(data)
is.factor(treatment)
is.ordered(treatment) # is factor but not ordered 

contrasts(data$treatment) # contr.treatment

treatment <- ordered(treatment, levels = c("none","low","medium","high"))
levels(treatment)
contrasts(treatment) # contr.poly: orthogonal polynomial contrasts 


model2 <- lm(response ~ treatment)
summary.aov(model2)

summary.lm(model2)


# fit polynomial regression model to the mean values of the response with the four ordered levels
yv <- as.vector(tapply(response, treatment, mean))
x <- 1:4
model <- lm(yv ~ x + I(x^2) + I(x^3))
summary(model)


x <- 1:4
x2 <- x^2
x3 <- x^3
cor(cbind(x, x2,x3))

t(contrasts(treatment)) # linear, quadratic and cubic 


# draw barplot to see how the curve looks like 
y <- as.vector(tapply(response, treatment, mean))
model <- lm(y ~ poly(x, 3))
summary(model)

xv <- seq(1, 4, 0.1)
yv <- predict(model, list(x=xv))

(bar.x <- barplot(y)) 
barplot(y, names = levels(treatment))
barplot(y)
xs <- -0.5 + 1.2 * xv 
lines(xs,yv,col="red")

rm(y)
detach(data)
``` 

**Summary for statistical modeling**
1. Data description for possible errors and outliers 

2. Model specification 

3. Check if there is no pseudoreplication, or need to specify appropriate random effects 

4. Fit models and do model cheking and model simplification. 



# Chapter 10 Regression 

**Important kinds of regression**: 
1. linear regression

2. polynomial regression for non-liearity relationship 

3. piecewise regression for two or more adjacent straight lines 

4. robust regression for less sensitivity to outliers 

5. multiple regression for many explanatory variables 

6. non-linear regression for a specified non-linear model 

7. non-parametric regression for data when there is no obvious functional form. 


The essence of regression analysis is using sample data to estimate paramter values and their standard errors. 


## Linear regression 

Assumptions: 

* variance of y is constant 

* x is measured with error 

* residuals are normally distributed 


```{r}
reg.data <- read.table("regression.txt",header=T) 
attach(reg.data)
names(reg.data)

# fit a linear regression
model <- lm(growth ~ tannin)
plot(tannin,growth, pch=21, col="blue", bg="red")
abline(model, col="red")
yhat <- predict(model,tannin = tannin)

join <- function(i)
 lines(c(tannin[i],tannin[i]),c(growth[i],yhat[i]),col="green")

# apply the function to all data points
sapply(1:9,join)


# sum of squares of residuals 
# null model
ssy <- deviance(lm(growth ~ 1))
ssy
# linear model 
sse <- deviance(lm(growth  ~ tannin))
sse
# then the R-square is 
r_square <- (ssy - sse)/ssy 
r_square

# absolute value correlation coefficient 
r <- sqrt(r_square)
r # while the sign is determined by the sign of EXY


# analysis of variance 
anova(lm(growth ~ tannin))

# confidence interval for coefficients 
confint(model)


# model checking 
par(mfrow = c(2, 2))
plot(model)
par(mfrow = c(1, 1))

model2 <- update(model,subset=(tannin != 6))
summary(model2)
detach(reg.data)
```


## Polynomial approximations to elementary functions 

Elementary functions expressed as Maclaurin series: 

1. $sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots$

2. $cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!}-\frac{x^6}{6!} + \cdots$

3. $exp(x) = 1 + \frac{x}{1!} + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots$

4. $log(x + 1) = x - \frac{x^2}{2} + \frac{x^3}{3}-\frac{x^4}{4} + \cdots$


```{r}
# approximation of sin(x) 

x <- seq(0,pi,0.01)
y <- sin(x)

# the original sin(x) curve
plot(x,y,type="l",ylab="sin(x)")

# approximation by the first two terms
a1 <- x - x^3/factorial(3)
lines(x, a1, col = "green")

# by the first three terms
a2 <- x - x^3/factorial(3) + x^5/factorial(5)
lines(x, a2, col = "red")
# good appriximations for small values 
rm(list = c("x", "y"))
``` 


## Polynomial regression 

```{r}
rm(list = c("xv", "yv")) # remove possible variables with the same names used below
poly <- read.table("diminish.txt",header=T) 
attach(poly)
names(poly)
par(mfrow=c(1,2))

# linear model 
model1 <- lm(yv ~ xv)
plot(xv, yv, pch=21, col = "brown", bg = "yellow")
abline(model1, col="navy")

# quadratic 
model2 <- lm(yv ~ xv + I(xv^2))

plot(xv, yv, pch=21, col = "brown", bg = "yellow")
x <- 0:90
y <- predict(model2, list(xv = x))
lines(x, y, col = "navy")
par(mfrow = c(1, 1))

# F test to see if the effect of the quadratic term is significant
anova(model1,model2) # significant

rm(list = c("x", "y")) # , "xv", "yv"))
detach(poly)
``` 

## Linear regression after transformation 

The most frequent transformations are logarithms, antilogs and reciprocals. 

```{r}
power <- read.table("power.txt",header = TRUE) 
attach(power)
names(power)

par(mfrow = c(1, 2))
# plot on original scale 
plot(area, response, pch = 21, col = "green", bg = "orange")
abline(lm(response ~ area), col = "blue")
# plot on log scale 
plot(log(area), log(response), pch = 21, col = "green", bg = "orange")
abline(lm(log(response) ~ log(area)), col = "blue")
par(mfrow = c(1, 1))

# linear model on original scale 
model1 <- lm(response ~ area)

# log scale 
model2 <- lm(log(response) ~ log(area))
summary(model2)

# a visual comparison of two models
plot(area, response, xlim = c(0, 5), ylim = c(0, 4), pch = 21, col = "green", bg = "orange")
abline(lm(response ~ area), col = "blue")
xv <- seq(0, 5, 0.01)
# log y = a + b logx , y = exp(a)*exp(logx^b) = exp(a) * x^b
yv <- exp(coef(model2)[1])*xv^coef(model2)[2]
lines(xv, yv, col = "red")
# more data will be helpful for choosing models 

rm(list = c("xv", "yv"))
detach(power)
```

## Prediction following regression 

**Extrapolation** : prediction beyond the measured range of the data 

**Interpolation** : predition within the measured range of the data, can often be accurate and not affected by model choice. 

```{r}
attach(reg.data)
plot(tannin, growth, pch = 21, col = "blue", bg = "red")
model <- lm(growth ~ tannin)
abline(model, col = "blue")

coef(model)[2] # b1
names(summary(model))
summary(model)[[4]][4] # standard error of b1


# add standard error lines for ONE standard error 
se.lines <- function(model){
b1 <- coef(model)[2] + summary(model)[[4]][4]
b2 <- coef(model)[2] - summary(model)[[4]][4] 

# model[[12]] is the original data set and [2] means the x values tannin 
xm <- sapply(model[[12]][2], mean) # mean tannin value
ym <- sapply(model[[12]][1], mean) # mean response growth value 
a1 <- ym - b1 * xm
a2 <- ym - b2 * xm
abline(a1, b1,lty=2,col="blue")
abline(a2,b2,lty=2,col="blue")
polygon(c(rev(tannin), tannin), c(rev(a1 + b1 * tannin), a2 + b2* tannin), col = "lavender", border = NA)
}

se.lines(model)


# add confidence intervals 

ci.lines <- function(model){
xm <- sapply(model[[12]][2], mean) # the overall mean for tannin (x)
n <- sapply(model[[12]][2], length) # total number of data 
ssx <- sum(model[[12]][2]^2)-sum(model[[12]][2])^2/n  # model[[12]] is original data 
s.t <- qt(0.975, (n-2)) # construct the 95% confidence interval from t distribution 
xv <- seq(min(model[[12]][2]), max(model[[12]][2]), length=100) # sequence depends on max and min x values 
yv <- coef(model)[1] + coef(model)[2]*xv
se <- sqrt(summary(model)[[6]]^2*(1/n+(xv - xm)^2/ssx))  # summary(model)[[6]] for sigma 
ci <- s.t*se
uyv <- yv+ci
lyv <- yv-ci
lines(xv,uyv,lty=2,col="blue")
lines(xv,lyv,lty=2,col="blue")
polygon(c(rev(xv), xv), c(rev(uyv), lyv), 
        col = rgb(1, 0.3, 0.2, alpha = 0.5),
        border = NA) # use rgb to specify a color with transparency
# rgb(red, green, blue, alpha, names = NULL, maxColorValue = 1)
}

plot(tannin,growth,pch=21,col="blue",bg="red")
abline(model, col="blue")
ci.lines(model)


# speed up the intervals drawing by using int = "c" and matlines 
plot(tannin, growth, pch=16, ylim=c(0, 15))
model <- lm(growth ~ tannin)

xv <- seq(0,8,0.1)
yv <- predict(model, list(tannin=xv), int="c")
matlines(xv, yv, lty=c(1, 2, 2), col = "black", border = NA)


detach(reg.data)
``` 

## Testing for lack of fit in a regression 

```{r}

data <- read.delim("lackoffit.txt")
names(data)
attach(data)
plot(conc, jitter(rate), pch = 16, col = "red", 
     ylim = c(0, 8), ylab = "Rate")
abline(lm(rate ~ conc), col = "blue")


model.reg <- lm(rate ~ conc)
summary(model.reg)


# pure error variance by setting each level a factor 
fac.conc <- factor(conc)
model.aov <- aov(rate ~ fac.conc)
summary(model.aov)

# lack of fit 
anova(model.reg, model.aov)

# a single ANOVA table showing the lack-of-fit sum of squares by fitting both models 
anova(lm(rate ~ conc + fac.conc))

# a visual impression of this lack of fit 

# means for each level 
my <- as.vector(tapply(rate, fac.conc, mean))

for (i in 0:6) lines(c(i,i),c(my[i+1], predict(model.reg, list(conc = 0:6))[i+1]), col = "green")

points(0:6, my, pch = 16, col = "green")

detach(data)
``` 

## Bootstrap with regression , another way is **jackknife** 

An alternative to estimate confidence intervals. 

** Two ways of doing bootstrapping**: 

1. sample cases with replacement, so some points are left off the graph while others appear more than once in the dataframe. 

2. calculate the residuals from the fitting regression model, and randomize which fitted y values get which residuals. 


```{r}
regdat <- read.table("regdat.txt", header = TRUE)
names(regdat)
rm(response)
attach(regdat)
plot(explanatory, response, pch = 21, col = "green", bg = "red")
model <- lm(response ~ explanatory)
abline(model, col = "blue")
model

# confidence interval from bootstrap 
b.boot <- numeric(10000)
for(i in 1:10000){
  indices <- sample(1:length(response), replace = TRUE)
  xv <- explanatory[indices]
  yv <- response[indices]
  model <- lm(yv ~ xv)
  b.boot[i] <- coef(model)[2]
}

hist(b.boot, main = "", col = "green")

# 95% interval for the bootstrapped estimate of the slope 
quantile(b.boot, c(0.025, 0.975))



# repeat the above exercise using the boot function 

library(boot)
reg.boot <- function(regdat, index){
  xv <- explanatory[index]
  yv <- response[index]
  model <- lm(yv ~ xv)
  coef(model)
}

reg.model <- boot(regdat, reg.boot, R = 10000)
boot.ci(reg.model, index = 2) # index indicates the position of the variable of interest 


# randomize the allocation of the residuals to fitted y values estimated from the original regression 

model <- lm(response ~ explanatory)
fit <- fitted(model) # fitted values 
res <- resid(model) # residuals 

# function used for bootstrapping 
residual.boot <- function(res, index){
y <- fit + res[index]
model <- lm(y ~ explanatory) 
coef(model) }

res.model <- boot(res, residual.boot, R = 10000)
boot.ci(res.model, index = 2)

``` 


## Jackknife

Each point in the data set is left out, one at a time and the paramter of interest is re-estimated. For more details, see [reference](https://en.wikipedia.org/wiki/Jackknife_resampling). 

```{r}
jack.reg <- numeric(length(response))


# carry out the regression 35 times leaving out one pair of x and y values 

for(i in 1:35){
  model <- lm(response[-i] ~ explanatory[-i])
  jack.reg[i] <- coef(model)[2]
}

hist(jack.reg, main = "", col = "pink") # heavily left skewed 

# check which point is the most influential
# in this case it's the point which caused the extreme left hand bar 


# extract Cook's Distance by infmat[, 5]
model <- lm(response ~ explanatory)
which(influence.measures(model)$infmat[, 5] == max(influence.measures(model)$infmat[, 5]))

# plot the data and do regresion without this point 
plot(explanatory ,response, pch = 21, col = "green", bg = "red")
abline(model, col = "blue")
abline(lm(response[-22] ~ explanatory[-22]), col = "red") # no big difference 
detach(regdat)
```

## Jackknife after bootstrap 

`jack.after.boot` calculates the jacknife influence values from a bootstrap output object, and plots the corresponding jackknife after bootstrap plot. 


```{r}
jack.after.boot(reg.model, index = 2)
# reg.model <- boot(regdat, reg.boot, R = 10000) from above 

# The centred jackknife quantiles for each observation are estimated from those bootstrap samples 
# These are then plotted against the influence values. 
# From the top downwards, the horizontal dotted lines show the 95th, 90th, 84th, 50th, 16th, 10th and 5th per- centiles. # The numbers at the bottom identify the 35 points by their index values within regdat.
# the influence of point no. 22 shows up clearly (this time on the right-hand side), 
# indicating that it has a strong positive influence on the slope, 
# and the two left-hand outliers are identified as points nos 34 and 30.
``` 


## Serial correlation in the residuals 

`durbinWatsonTest`, the Durbin-Watson function is used for testing whether there is autocorrelation in the residuals from a linear model or a generalized linear model. 


```{r}
library(car)
durbinWatsonTest(model)
# no evidence of serial correlation 
```

## Piecewise regression 

Two problems to be solved:

1. How many segments needed? 

2. Where are the break points on the x axis? 


```{r}
data <- read.table("sasilwood.txt", header = TRUE)
attach(data)
names(data)

plot(log(Species) ~ log(Area), pch = 21, col = "red", bg = "lavender")

# check the model 
model1 <- lm(log(Species) ~ log(Area))
par(mfrow = c(2, 2))
plot(model1) # not good 
par(mfrow = c(1, 1))

# check where to break given that using two segments 
table(Area)

# include a logical statement as part of the model formula for piecewise regression 

# check the break points of each Area and find the one with minimum standard error 
Break <- sort(unique(Area))[3:11] # check the middle ones  
Break
d <- numeric(length(Break))
for(i in 1:length(Break)){
  model <- lm(log(Species) ~ (Area < Break[i]) * log(Area) + 
                (Area >= Break[i]) * log(Area))
  d[i] <- summary(model)$sigma
}

# location to have break with smallest sigma 
which(d == min(d))
Break[which(d == min(d))] 

# fit model with corresponding break 
model2 <- lm(log(Species) ~ log(Area) * (Area < 100) + log(Area) * (Area >= 100))

anova(model1, model2)

summary(model2)$coef

# visulization 

a1 <- summary(model2)[[4]][1] + summary(model2)[[4]][3]
a2 <- summary(model2)[[4]][1]
b1 <- summary(model2)[[4]][2] + summary(model2)[[4]][4]
b2 <- summary(model2)[[4]][2]

plot(log(Area), log(Species), col="blue", xlab = "Log(Area)", ylab = "log(Species")
lines(c(-5, 4.6), c(a1 + b1*-5, a1 + b1*4.6), col = "red")
lines(c(4.6, 15), c(a2 + b2*4.6, a2 + b2*15), col = "red")

detach(data)
``` 

## Multiple regression 

Possible problems: 

* over fitting 

* parameter proliferation due to curvature, interaction ... 

* multi-colinearity 

* non-independence of groups of measurements 

* temporal or spatial correlation amongst the explanatory variables 

* pseudoreplication 



```{r}

ozone.pollution <- read.table("ozone.data.txt", header = TRUE) 
attach(ozone.pollution)
names(ozone.pollution)

# scatterplot matrix for a visual check about correlations 
pairs(ozone.pollution,panel = panel.smooth)

# use non-parametric smoothers in generalized additive models 
library(mgcv)
par(mfrow = c(2,2))
model <- gam(ozone ~ s(rad) + s(temp) + s(wind))
plot(model) # check if the confidence intervals are narrow or wide 


# fit tree model to figure out which factor(s) is(are) important and see 
# the interactions between variables 
library(tree)
model <- tree(ozone ~ ., data = ozone.pollution)
par(mfrow=c(1,1))
plot(model)
text(model)


# initial complex model to begin with 
w2 <- wind^2
t2 <- temp^2
r2 <- rad^2
tw <- temp*wind
wr <- wind*rad
tr <- temp*rad
wtr <- wind*temp*rad

model1 <- lm(ozone ~ rad + temp + wind + t2 + w2 + r2 + wr + tr + tw + wtr)
summary(model1)

# remove wtr 
model2 <- update(model1, ~ .-wtr)
summary(model2)

model3 <- update(model2, ~ .-r2)
summary(model3)

model4 <- update(model3, ~ .-tr)
summary(model4)


model5 <- update(model4, ~ .-tw)
summary(model5)


model6 <- update(model5, ~ .-wr)
summary(model6)

par(mfrow=c(2,2))
plot(model6)

model7 <- lm(log(ozone) ~ rad+temp+wind+t2+w2+r2+wr+tr+tw+wtr)
summary(model7)

model8 <- update(model7,~.-wtr)
summary(model8)
model9 <- update(model8,~.-tr)
summary(model9)
model10 <- update(model9,~.-tw)
summary(model10)
model11 <- update(model10,~.-t2)
summary(model11)
model12 <- update(model11,~.-wr)
summary(model12)

plot(model12) # minimum adequate 


par(mfrow = c(1, 1))
detach(ozone.pollution)
``` 








