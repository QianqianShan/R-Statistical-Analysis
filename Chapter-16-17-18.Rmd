---
title: Chapter 16 Proportion data | Chapter 17 Binary response variables | Chapter
  18 Generalized additive models
author: "Qianqian Shan"
date: "June 7, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "therbookdata")
```

# Chapater 16 Proportion Data 

Count data on proportions. 

## Analysis of data on one and two proportions 

* Comparisons of one binomial proportion with a constant , use `binom.test`. 

* Comparison of two samples of proportion data, use `prop.test`. 

$ln(\frac{p}{1-p}) = a + bx$ wiht a linear predictor, logit transformation of $p$. 


```{r}
# logistic regression with binomial errors 
numbers <- read.table("sexratio.txt", header = TRUE)
attach(numbers)
head(numbers)

# overview of data 
par(mfrow=c(1,2))
# male ratio
p <- males/(males + females)
plot(density, p, ylab = "Proportion male", pch = 16, col = "blue")
# log(density)
plot(log(density), p, ylab = "Proportion male", pch = 16, col = "blue")
par(mfrow= c(1, 2))

# glm with binomial errors 
y <- cbind(males, females)

model <- glm(y ~ density, family = binomial)

summary(model)
# there is substantial overdipsersion as deviance is much bigger than the df 


# fit log density
model2 <- glm(y ~ log(density), family = binomial)
summary(model2)

par(mfrow = c(1, 1))

# plot the fitted line 
xv <- seq(0, 6, 0.01)
yv <- predict(model2, list(density = exp(xv)), type = "response") # type = response 
plot(log(density), p, ylab = "Proportion male", pch = 16, col = "blue")
lines(xv, yv, col = "red")

detach(numbers)



# we want to know what kills the 50%(y, dead) , 
# i.e., use y to predict x and work out  a standard error on the x axis 
data <- read.table("bioassay.txt", header = TRUE) 
attach(data)
head(data)

y <- cbind(dead, batch - dead)
model <- glm(y ~ log(dose), family = binomial)

library(MASS)
dose.p(model, p = c(0.5, 0.9, 0.95))
# dose.p(obj, cf = 1:2, p = 0.5)
# dose.p calibrates binomial assays, generalizing the calculation of LD50.
detach(data)


# proportion data with categorical explanatory variabels 
germination <- read.table("germination.txt", header = TRUE)
attach(germination)
names(germination)

y <- cbind(count, sample - count)

levels(Orobanche)

levels(extract)


# factorial analysis 
model <- glm(y ~ Orobanche * extract, binomial)
summary(model)

# approximate dispersion parameter 
sum(summary(model)$deviance.resid^2)/summary(model)$df.residual


# use quasi-binomial 
model <- glm(y ~ Orobanche * extract, family = quasibinomial)
summary(model)

# update model 
model2 <- update(model, ~ . - Orobanche:extract)

anova(model, model2, test = "F")

anova(model2, test = "F")


# Orobanche factor seems not significant in model2
model3 <- update(model2, ~ . - Orobanche)
anova(model2, model3, test = "F") # minimal adequate 

coef(model3)
a <- coef(model3)[1]
b <- coef(model3)[2]

# the p for the first extract type 
1/(1+1/(exp(a)))

# p for the second extract type 
1/(1+1/(exp(a + b)))

# make prediction 
tapply(predict(model3, type = "response"), extract, mean)


# the average of raw proportions 
as.vector(tapply(count,extract,sum))/as.vector(tapply(sample,extract,sum))
# The average of proportions is the total counts over the total samples, 
# NOT averaging the raw proportions one by one 

detach(germination)
```


## Analysis of covariance with binomial data 

Data with both continuous and categorical explanatory variables. 


```{r}

props <- read.table("flowering.txt", header = TRUE) 
attach(props)
names(props)
# dose continuous, variety categorical 
y <- cbind(flowered, number - flowered)
pf <- flowered/number
pfc <- split(pf, variety)
dc <- split(dose, variety)
plot(dose, pf, type = "n", ylab = "Proportion flowered")
points(jitter(dc[[1]]), jitter(pfc[[1]]), pch = 21, col = "blue", bg = "red")
points(jitter(dc[[2]]), jitter(pfc[[2]]), pch = 21, col = "blue", bg = "green")

points(jitter(dc[[3]]), jitter(pfc[[3]]), pch = 21, col = "blue", bg = "yellow")
points(jitter(dc[[4]]), jitter(pfc[[4]]), pch = 21, col = "blue", bg = "green3")
points(jitter(dc[[5]]), jitter(pfc[[5]]), pch = 21, col = "blue", bg = "brown")


# fit maximal model 
model1 <- glm(y ~ dose * variety, family = binomial)
summary(model1) # overdispersion 

# plot fitted curve 
xv <- seq(0, 35, 0.1)
vn <- rep("A", length(xv))
yv <- predict(model1, list(variety = factor(vn), dose = xv), type = "response")
lines(xv, yv, col = "red")
vn <- rep("B", length(xv))
yv <- predict(model1, list(variety = factor(vn), dose = xv), type = "response")
lines(xv, yv, col = "green")
vn <- rep("C", length(xv))
yv <- predict(model1, list(variety = factor(vn), dose = xv), type = "response")
lines(xv, yv, col = "yellow")
vn <- rep("D", length(xv))
yv <- predict(model1, list(variety = factor(vn), dose = xv), type = "response")
lines(xv, yv, col = "green3")

vn <- rep("E", length(xv))
yv <- predict(model1, list(variety = factor(vn), dose = xv), type = "response")
lines(xv, yv, col = "brown")
legend("topleft", legend = c("A", "B", "C", "D", "E"), title = "variety",
            lty = rep(1, 5), col = c("red", "green", "yellow", "green3", "brown"))


tapply(pf, list(dose, variety), mean)

detach(props)
```

**Sumamry**: we have proportion data doesn't necessarily mean that the data will be well described by the logistic model. 


## Converting complex contingency tables to proportions

Remove the need for all of the nuisance variables that are involved in complex contingency table modeling.



```{r}
lizards <- read.table("lizards.txt", header = TRUE)
attach(lizards)
head(lizards)

sorted <- lizards[order(species, sun, height, perch, time), ]
levels(species) # two levels 
head(sorted)
dim(sorted) # 1-24 one species, 25-48 another species 

short <- sorted[1:24, ]

names(short)[1] <- "Ag" # the original "n" column 
names(short)
head(short)

# delete the last column, i.e., the species 
short <- short[, -6]
head(short)

new.lizards <- data.frame(sorted$n[25:48], short)

names(new.lizards)[1] <- "Ao"
head(new.lizards)
# create new columns Ao Ag to replace the original "n" column 
# deleted the speices column 

detach(lizards)
rm(short, sorted)
attach(new.lizards)

names(new.lizards)

y <- cbind(Ao, Ag)

model1 <- glm(y ~ sun * height * perch * time, family = binomial)

model2 <- step(model1)

model3 <- update(model2,~. - height:perch:time)
model4 <- update(model2,~. - sun:height:perch)
anova(model2,model3,test="Chi")

anova(model2,model4,test="Chi")

model5 <- glm(y~(sun+height+perch+time)^2-sun:time,binomial)

model6 <- update(model5,~. - sun:height)
anova(model5,model6,test="Chi")

model7 <- update(model5,~. - sun:perch)
anova(model5,model7,test="Chi")


model8 <- update(model5,~. - height:perch)
anova(model5,model8,test="Chi")


model9 <- update(model5,~. - time:perch)
anova(model5,model9,test="Chi")

model10 <- update(model5,~. - time:height)
anova(model5,model10,test="Chi")

model11 <- glm(y~sun+height+perch+time,binomial)
summary(model11)


# combine levels 
t2 <- time
levels(t2)[c(2,3)] <- "other"
levels(t2)



model12 <- glm(y~sun+height+perch+t2,binomial)
anova(model11,model12,test="Chi")

summary(model12)
detach(new.lizards)
rm(y)
```




# Chapter 17 Binary response variables 

Steps: 

1. Create a single vector containing 0s and 1s as response variables.

2. Use `glm` with `family = binomial`. 

3. Consider changing the link function from default logit to complementary log-log. 

4. Fit the model in the usual way. 

5. Test significance by deletion of terms from the maximal model, and compare the change in deviance with chi-squared. 




```{r}
island <- read.table("isolation.txt", header = TRUE)
attach(island)
names(island)
# incidence is 1 or 0

# maximal 
model1 <- glm(incidence ~ area * isolation, family = binomial)

# w/o interaction 
model2 <- glm(incidence ~ area + isolation, family = binomial)

anova(model1, model2, test = "Chi")

summary(model2)



# plot fitted lines against each separately variable 
modela <- glm(incidence ~ area, family = binomial)
modeli <- glm(incidence ~ isolation, family = binomial)

par(mfrow=c(1, 2))
xv <- seq(0, 9, 0.01)
yv <- predict(modela, list(area = xv), type = "response")
plot(area, incidence)
lines(xv, yv, col = "red")
xv2 <- seq(0, 10, 0.01)
yv2 <- predict(modeli, list(isolation = xv2), type = "response")
plot(isolation, incidence)
lines(xv2, yv2, col = "red")

plot(isolation, incidence)
lines(isolation[order(isolation)], predict(model2, type = "response")[order(isolation)])
plot(area, incidence)
lines(area[order(area)], predict(model2, type = "response")[order(area)])
par(mfrow = c(1, 1))




library(scatterplot3d)


s3d <- scatterplot3d(x = isolation, y = area, z = predict(model2, type = "response"), 
                     pch = 16, highlight.3d = TRUE, type = "h", zlab = "p")
detach(island)
```


## Graphical tests of the fit of the logistic 

**Rugs** are one-dimentional addition to the bottom(or top) of the plot showing the locations of the data points along x axis. 

```{r}
occupy <- read.table("occupation.txt", header = TRUE) 
attach(occupy)
names(occupy)


# use of rug 
with(faithful, {
    plot(density(eruptions, bw = 0.15))
    rug(eruptions)
    rug(jitter(eruptions, amount = 0.01), side = 3, col = "light blue")
})


plot(resources, occupied, type = "n")
rug(jitter(resources[occupied == 0]))
rug(jitter(resources[occupied == 1]), side = 3)

model <- glm(occupied ~ resources, family = binomial)
xv <- 0:1000
yv <- predict(model, list(resources = xv), type = "response")
lines(xv, yv, col = "red")

# cut up the ranked values on x axis into five categories and 
# then work out the mean and standard error of the proportions 
# of each group 
cutr <- cut(resources, 5)
head(cutr)
tapply(occupied, cutr, sum) # number of observations in each group 

table(cutr)

# empirical probabilities 
probs <- tapply(occupied, cutr, sum)/table(cutr)
probs

probs <- as.vector(probs)

# mean values of each group as the x values of the empirical probabilities 
resmeans <- tapply(resources, cutr, mean)
resmeans <- as.vector(resmeans)

points(resmeans, probs, pch = 16, cex = 2, col = "blue")

# standard error of each point by se = sqrt(prob * (1 - prob) / n)
se <- sqrt(probs * (1 - probs)/table(cutr))

up <- probs + as.vector(se)
down <- probs - as.vector(se)
for (i in 1:5) {
  lines(c(resmeans[i], resmeans[i]), c(up[i], down[i]), col = "blue")
}


detach(occupy)

```

## ANCOVA with binary response variable 


```{r}
infection <- read.table("infection.txt", header = TRUE) 
attach(infection)
names(infection)
# infected is binary response 
# age , weight are continous 
# sex categorical 

par(mfrow=c(1,2))
plot(infected, weight, xlab = "Infection", ylab = "Weight", col = "green")
plot(infected, age, xlab = "Infection", ylab = "Age", col = "green4")
par(mfrow = c(1, 1))

# relationship with gender 
table(infected, sex)

# maximal model 
model <- glm(infected ~ age * weight * sex, family = binomial)
summary(model)


# use step 
model2 <- step(model)

summary(model2)


# interactions not significant, use update to simplify 
model3 <- update(model2, ~.-age:weight)
anova(model2, model3, test = "Chi")

# 
model4 <- update(model2, ~.-age:sex)
anova(model2, model4, test = "Chi")

# test the main effects 
model5 <- glm(infected ~ age + weight + sex, family = binomial)
summary(model5)
# age is not significant in the overall model, however, is marginally significant 


# fit quadratic terms for the continous variables to test non-linearity 
model6 <- glm(infected ~ age + weight + sex + I(weight^2) + I(age^2), family = binomial)
summary(model6) # significant 




# looking at the non-linearities in more detail, 

# see if we can do better with other kinds of models such as 
# non-parametric smoothers, piecewise linear models or step functions 


# gam 
library(mgcv)
model7 <- gam(infected ~ sex + s(age) + s(weight), family = binomial)
par(mfrow=c(1,2))
plot.gam(model7)
par(mfrow = c(1, 1))

# piecewise linear with threshold from above plots by lowest residual deviance 
model8 <- glm(infected ~ sex + age + I(age^2) + I((weight - 12) * (weight > 12)),
              family = binomial)
summary(model8)

# minimal adequate 
model9 <- glm(infected ~ age + I(age^2) + I((weight - 12) * (weight > 12)), family = binomial)
summary(model9)
detach(infection)
```


## Binary response with pseudoreplication 

* General linear mixed effects model 

* Only use the data measured the last (or any specified) 

* Convert to proportions and use binomial or quasi-binomial family within glm 


```{r}
library(MASS)
attach(bacteria)
names(bacteria)

table(y)
# yes or no for infection 

table(y, trt) # three treatments 

#  random effects defined by the round brackets 
# and the "given" operator to separate the continuous 
# random effect(week) from the categorical random effect
# (ID) 
library(lme4)
model1 <- glmer(y ~ trt + (week | ID), family = binomial)
summary(model1)
# week random effect not significant 
# fixed effects not significant 

# remove the dependence of infection on week 
model2 <- glmer(y ~ trt + (1|ID), family = binomial)
anova(model1, model2)
# accept model1 


# combine drug and drug+
drugs <- factor(1 + (trt != "placebo"))
table(y, drugs)

model3 <- glmer(y ~ drugs + (week|ID), family = binomial)
summary(model3)
# sample size too small to demonstrate the significance of its efficiency 

table(y, trt)

# wrong way to do proportion test due to the pseudo replication 
# as seen above, the effect is not significant, however significant here 
prop.test(c(12, 18, 13), c(96, 62, 62)) # the second argument is the total 


# one way to deal with pseudo replication is to only 
# use the data from the end of the experiment 

# check if there are obs that are measured twice within a week 
head(table(ID, week))
any(table(ID, week) > 1) # no 

# fit model with a subset of data 
model <- glm(y ~ trt, family = binomial, subset = (week == 11))
summary(model)

# combine drug levels 
drugs <- factor(1 + (trt == "placebo"))


table(drugs[week == 11])

# refit 
model <- glm(y ~ drugs, family = binomial, subset = (week == 11))
summary(model)
# not significant drug effect 



# convert the data into proportions so each patient have one proportion 
dss <- data.frame(table(trt, ID))
head(dss)

# only select the treatment and patients combination with Freq > 0
tss <- dss[dss[, 3] > 0, ]$trt
ys <- table(y, ID)
yv <- cbind(ys[2, ], ys[1, ])


# fit 
model <- glm(yv ~ tss, family = binomial)
summary(model)
# overdispersion 

# refit 
model <- glm(yv ~ tss, family = quasibinomial)
summary(model)


# combine two drug effects 
tss2 <- factor(1 + (tss == "placebo"))
model <- glm(yv ~ tss2, family = quasibinomial)
summary(model)
# no significant drug effects, consistent with mixed effects models 
detach(bacteria)

```


# Chapter 18 Generalized Additive Models 

Useful when we have no a *priori* reason to choose a particular parametric model. 

All error families allowed with `glm` are available, `update` , `predict`, `summary`, `anova` and so on are also available. 

* s(x, z) will do isotropic smooth. 

* s(x, z) + s(z, w) is allowed for overlapping terms. 

* te(x, z, k = 6) (example k) smooths interactions of any number of variables via scale invariant tensor product smooths. 

* s(z, bs = "cr", k = 6) (example) do smoothing with cubic regression spline(cr), while the default is "tp". 

**Technical aspects**: 

* The degree of smoothness of model terms is estimated as part of the fitting 

* Isotropic or scale-invariant smooths of any number of variables are available as model terms 

* Confidence or credible intervals are readily available for any quantity predicted using a fitted model 

* In `mgcv`, `gam` solves the smoothing parameter estimation by using 

1. the generalized cross validation(GCV): $GCV = \frac{nD}{(n - d.f.)^2}$. 

2. unbiased risk estimator(UBRE) when $\phi$ is known: $UBRE = \frac{D}{n} + 2\phi \frac{d.f.}{n} - \phi$. 

See `?gam.method` for more details. 



## Non-parametric smoothers 

* `loess` 

* `tree` 


```{r}
soay <- read.table("soaysheep.txt", header = TRUE) 
attach(soay)
names(soay)
# Delta is the yearly change, population is the density 


plot(Population, Delta, pch = 21, col = "green", bg = "red")

model <- loess(Delta ~ Population) 
# loess : Fit a polynomial surface determined by one or more numerical predictors, using local fitting.

summary(model)

# draw smoothed line 
xv <- seq(600, 2000, 1)
yv <- predict(model, data.frame(Population = xv))
lines(xv, yv, col = "red") # looks like a step function 

rm(xv, yv)


# use tree to determine the threshold for splitting the data into low and high density parts 
library(tree)
thresh <- tree(Delta ~ Population)
print(thresh)
# plot(thresh)

th <- 1289.5 # threshold 


model2 <- aov(Delta ~ (Population > th))
summary(model2)

tail(Delta, 2) # the 45th data is NA , remove it 
tapply(Delta[-45], (Population[-45] > th), mean)


# add step functions 
lines(c(600, th), c(0.2265, 0.2265), lty = 2, col = "blue")
lines(c(th, 2000), c(-0.2837, -0.2837), lty = 2, col = "blue")
lines(c(th, th), c(-0.2837, 0.2265), lty = 2, col = "blue")


# Three parameters (two averages and a threshold) in step function,
# 4.66 df for loess, 
# parsimony favours the step function 
detach(soay)

```

## Generalized additive models 

`gam` is used. 


```{r}
ozone.data <- read.table("ozone.data.txt", header = TRUE)
attach(ozone.data)
names(ozone.data)
# ozone is y , the other three are continuous variables 


# inspect the data with non parametric loess 
pairs(ozone.data, panel = function(x, y) {points(x, y, pch = 16, cex = 0.6); lines(lowess(x, y), col = "red")} )

# fit all variables with non parametric smoothers s()
# s() does not evaluate a (spline) smooth - it exists purely to help set up a model using spline based smooths.

model <- gam(ozone ~ s(rad) + s(temp) + s(wind))
summary(model)

# add interaction term using update
model2 <- update(model, ~ . + s(wind, temp))
summary(model2)
# write out the model 
model3 <- gam(ozone ~ s(temp) + s(wind) + s(rad) + s(wind,temp))
summary(model3)
anova(model2, model3) # these two models should be the same 

par(mfrow=c(2, 2))
plot(model3, residuals = TRUE, pch = 16)
par(mfrow = c(1, 1))

detach(ozone.data)
```


## An example with strongly humped data 

```{r}
# install.packages("SemiPar")
library(SemiPar)
data(ethanol)
attach(ethanol)
head(ethanol)
# NOx is y 

# fit E as smoothed term and C as parametric term 
model <- gam(NOx ~ s(E) + C)

par(mfrow=c(1,2))
plot.gam(model, residuals = T, pch = 16, all.terms = T)

coplot(NOx ~ C | E, panel = panel.smooth) # only panel 2 has a pronounced effect 
par(mfrow = c(1, 1))


# add interaction term without C 
CE <- E * C
model2 <- gam(NOx ~ s(E) + s(CE))


plot.gam(model2, residuals = TRUE, pch = 16, all.terms = T)

summary(model2) # significant 
detach(ethanol)
```

## Generalized additive models with binary data 


```{r}
attach(island)
names(island)

model3 <- gam(incidence ~ s(area) + s(isolation), family = binomial)
summary(model3) # area not significant 

par(mfrow=c(1, 2))
plot.gam(model3, residuals = TRUE, pch = 16)

# fit isolation alone 
model4 <- gam(incidence ~ s(isolation), family = binomial)
anova(model3, model4, test = "Chisq") # model3 preferred 


# fit area as parameteric term 
model5 <- gam(incidence ~ area + s(isolation), family = binomial)
summary(model5) # significant 
detach(island)
```

**Summary**: a term can appear to be significant when entered as a parametric term but not when as a non-parametric term. 


## Three-dimensional graphic output from `gam` 

`vis.gam` is used when there are two continuous explanatory variables. It produces perspective or contour plot views of gam model predictions, fixing all but the values in view to the values supplied in cond.

```{r}

test1 <- function(x, z, sx = 0.3, sz = 0.4) {
  (pi**sx*sz) * (1.2 * exp(- (x - 0.2)^2/sx^2 - (z - 0.3)^2/sz^2) +
                  0.8*exp(- (x - 0.7)^2/sx^2 - (z - 0.8)^2/sz^2))
}


n <- 500
x <- runif(n); z <- runif(n); # random variables from unif(0, 1)
y <- test1(x, z) + rnorm(n) * 0.1
b4 <- gam(y ~ s(x, z))
vis.gam(b4)
# z axis is the linear predictor 

``` 

